# Carbon Capture Network Logstash Pipeline Configuration

input {
  # Receive logs from Filebeat
  beats {
    port => 5044
    ssl => false
  }

  # Receive logs from Docker containers
  tcp {
    port => 5000
    codec => json_lines
    type => "docker_logs"
  }

  # Receive logs from syslog
  udp {
    port => 5000
    type => syslog
  }

  # MQTT logs from Mosquitto
  mqtt {
    broker_host => "mosquitto"
    broker_port => 1883
    topics => ["logs/#"]
    codec => json
    type => "mqtt_logs"
  }
}

filter {
  # Parse JSON logs
  if [type] == "docker_logs" or [type] == "mqtt_logs" {
    json {
      source => "message"
    }
  }

  # Parse application logs
  if [fields][service] == "carbon-capture-backend" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \[%{DATA:thread}\] %{DATA:class} - %{GREEDYDATA:message}" }
    }

    # Extract additional fields for backend logs
    if [level] == "ERROR" or [level] == "WARN" {
      mutate {
        add_field => { "severity" => "high" }
      }
    }
  }

  if [fields][service] == "carbon-capture-ai-engine" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{DATA:logger} - %{LOGLEVEL:level} - %{GREEDYDATA:message}" }
    }

    # Extract AI-specific metrics
    grok {
      match => { "message" => "Prediction completed in %{NUMBER:prediction_time:float} seconds for unit %{DATA:unit_id}" }
    }
  }

  # Parse MongoDB logs
  if [type] == "mongodb" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:severity} %{DATA:component} %{GREEDYDATA:message}" }
    }
  }

  # Parse Redis logs
  if [type] == "redis" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:level} %{GREEDYDATA:message}" }
    }
  }

  # Parse MQTT logs
  if [type] == "mqtt_logs" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{DATA:client_id} %{GREEDYDATA:message}" }
    }
  }

  # Add timestamp if not present
  if ![timestamp] {
    date {
      match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"]
      target => "@timestamp"
    }
  }

  # Add geoip information for IP addresses
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }

  # Add service-specific tags
  if [fields][service] {
    mutate {
      add_tag => ["service:%{[fields][service]}"]
    }
  }

  # Extract error patterns
  if [level] == "ERROR" or [level] == "WARN" {
    grok {
      match => { "message" => "Error: %{GREEDYDATA:error_message}" }
      tag_on_failure => ["_grokparsefailure"]
    }

    # Extract stack traces
    multiline {
      pattern => "^\s"
      what => "previous"
      negate => true
    }
  }

  # Add environment information
  mutate {
    add_field => { "environment" => "production" }
    add_field => { "application" => "carbon-capture-network" }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => ["beat", "host", "input", "offset", "@version"]
  }
}

output {
  # Send all logs to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "carbon-capture-%{+YYYY.MM.dd}"

    # Index template
    template => "/usr/share/logstash/config/elasticsearch-template.json"
    template_name => "carbon-capture-logs"
    template_overwrite => true

    # Authentication (if enabled)
    # user => "elastic"
    # password => "${ELASTIC_PASSWORD}"
  }

  # Send critical errors to stdout for debugging
  if [level] == "ERROR" and [severity] == "high" {
    stdout {
      codec => rubydebug
    }
  }

  # Send metrics to Prometheus (optional)
  # if [type] == "metrics" {
  #   prometheus {
  #     host => "pushgateway"
  #     port => 9091
  #     metric => "%{[fields][metric_name]}"
  #     value => "%{[fields][metric_value]}"
  #     labels => {
  #       service => "%{[fields][service]}"
  #       unit_id => "%{[fields][unit_id]}"
  #     }
  #   }
  # }
}
